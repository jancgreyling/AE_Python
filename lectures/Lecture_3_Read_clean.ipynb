{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "- Reading data is the first step to any data analysis task.\n",
    "- Pandas offers versatile functions to read data from various file formats.\n",
    "  \n",
    "  **Example for reading from an Excel file**:\n",
    "  ```python\n",
    "  df = pd.read_excel('input.xlsx', sheet_name='Sheet1')\n",
    "  df\n",
    "  ```\n",
    "\n",
    "  **Example for reading from a CSV file**:\n",
    "  ```python\n",
    "  df = pd.read_csv('data/raw/input.csv')\n",
    "  df\n",
    "  ```\n",
    "\n",
    "- **File Formats**:\n",
    "  - Pandas provides functions to read from a wide range of sources, including:\n",
    "    - Text formats such as CSV, JSON, and HTML. Use `pd.read_csv()`, `pd.read_json()`, and `pd.read_html()` respectively.\n",
    "    - Binary formats such as Excel, HDF5, and Parquet. Use `pd.read_excel()`, `pd.read_hdf()`, and `pd.read_parquet()` respectively.\n",
    "    - SQL databases like SQLite, PostgreSQL, and MySQL. For this, a connection needs to be established with the respective database and then use `pd.read_sql_query()` or `pd.read_sql_table()` to fetch the data.\n",
    "\n",
    "Remember, when reading data, you need to specify the correct filepath to the data that you want to read. The filepath is the location of the file on your computer. If you are using Google Colab, you need to:\n",
    "1. mount your drive\n",
    "2. set your working directory\n",
    "3. specify the filepath to the data on your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by importing the maize data and having a look at it - it comes from the South African Abstract of Agricultural Statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#set the working directory\n",
    "os.chdir(\"/Users/jancg/Library/CloudStorage/OneDrive-StellenboschUniversity/3_LE/3_Courses/AE_Python\")\n",
    "\n",
    "#read the data\n",
    "df_mz = pd.read_csv(\"data/raw/SA_maize.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported the data, we need to have a look at the data so check the data types and the first few rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prod year Area planted production      Value price white price yellow\n",
      "0   1980/81        4 488     14 872  1 768 711      134.15          134\n",
      "1   1981/82        4 664      8 781  1 190 204      155.05       155.05\n",
      "2   1982/83        4 680      4 399    770 447      170.05       170.05\n",
      "3   1983/84        4 839      4 797  1 055 662       219.5       215.55\n",
      "4   1984/85        4 502      8 444  1 920 603      221.45        217.5\n"
     ]
    }
   ],
   "source": [
    "print(df_mz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we've got a production year variable and then a some indicators for each year. A couple of observations:\n",
    "\n",
    "1. Some of the variable names have spaces in them, which is not ideal, we will need deal with the spaces\n",
    "2. ome of the column names have capital letters in them, which is also not ideal, as a rule of thumb, we should always use lower case letters for column names.\n",
    "3. it seems that some or all of the year values are separated by `/`, this can cause problems when we want to do calculations with the data. We need to address it.\n",
    "4. we need to check the data types since we want to do some calculations with the data and we need to make sure that the data types are correct. The fact that there spaces as 1000 separators in the data means that the data types are probably not numeric.\n",
    "5. here is a column called `value`, this is not a very descriptive name, we should change it to something more descriptive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. & 2.** Lets start by renaming the columns to remove the spaces. Lets just look for the spaces in the column names and replace them with `_`. Lets also deal with the capital letters at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prod_year', 'area_planted', 'production', 'value', 'price_white',\n",
      "       'price_yellow'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#replace spaces in column names with underscores\n",
    "df_mz.columns = df_mz.columns.str.replace(' ', '_')\n",
    "#all lower case\n",
    "df_mz.columns = df_mz.columns.str.lower()\n",
    "\n",
    "print(df_mz.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Now lets replace the `/` with a `_` in the production year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1980_81\n",
      "1    1981_82\n",
      "2    1982_83\n",
      "3    1983_84\n",
      "4    1984_85\n",
      "Name: prod_year, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_mz['prod_year'] = df_mz['prod_year'].str.replace('/', '_')\n",
    "print(df_mz['prod_year'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Now lets check the data types and takes the necessary steps if they are incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42 entries, 0 to 41\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   prod_year     42 non-null     object\n",
      " 1   area_planted  42 non-null     object\n",
      " 2   production    42 non-null     object\n",
      " 3   value         42 non-null     object\n",
      " 4   price_white   42 non-null     object\n",
      " 5   price_yellow  42 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_mz.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so they are objects which is not what we want, and we cannot convert them to numeric because of the spaces. Lets remove the spaces and convert the data types to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove spaces\n",
    "df_mz['area_planted'] = df_mz['area_planted'].str.replace(' ', '')\n",
    "df_mz['production'] = df_mz['production'].str.replace(' ', '')\n",
    "df_mz['value'] = df_mz['value'].str.replace(' ', '')\n",
    "df_mz['price_white'] = df_mz['price_white'].str.replace(' ', '')\n",
    "df_mz['price_yellow'] = df_mz['price_yellow'].str.replace(' ', '')\n",
    "\n",
    "#convert to numeric\n",
    "df_mz['area_planted'] = pd.to_numeric(df_mz['area_planted'])\n",
    "df_mz['production'] = pd.to_numeric(df_mz['production'])\n",
    "df_mz['value'] = pd.to_numeric(df_mz['value'])\n",
    "df_mz['price_white'] = pd.to_numeric(df_mz['price_white'])\n",
    "df_mz['price_yellow'] = pd.to_numeric(df_mz['price_yellow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Lastly, lets rename the `value` column to something more descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename value column\n",
    "df_mz = df_mz.rename(columns={'value': 'output_value_rands'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Yield\n",
    "\n",
    "Lets calculate the maize yield for each year and add it to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mz['yield'] = df_mz['production'] / df_mz['area_planted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the describe function, we can see summary statistics for the yield variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    42.000000\n",
       "mean      3.082552\n",
       "std       1.315369\n",
       "min       0.785286\n",
       "25%       2.210766\n",
       "50%       2.850034\n",
       "75%       3.958561\n",
       "max       5.860100\n",
       "Name: yield, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mz['yield'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the we have 42 years of data with an average yield of 3.08 tons per hectare. The minimum yield was 0.78 tons per hectare and the maximum yield was 5.8 tons per hectare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide and Long Data Types in Data Manipulation\n",
    "\n",
    "When working with data, especially in a tabular format like dataframes in Python, it's common to encounter two primary structures: **wide format** and **long format** (sometimes called \"tidy\" format). These structures refer to how data is organized in rows and columns. Understanding the distinction between the two is crucial for effective data manipulation and visualization in Python.\n",
    "\n",
    "### Wide Format\n",
    "\n",
    "In a **wide format**, a single subject's repeated observations are spread across multiple columns. It’s called ‘wide’ because you might have a wide table with many columns.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose we have a dataset tracking the sales of a product for three months: January, February, and March.\n",
    "\n",
    "```\n",
    "| Subject | January | February | March |\n",
    "|---------|---------|----------|-------|\n",
    "| Product A | 10      | 12       | 11    |\n",
    "| Product B | 15      | 17       | 18    |\n",
    "```\n",
    "\n",
    "Here, the sales numbers for each month are spread horizontally in separate columns.\n",
    "\n",
    "### Long (Tidy) Format\n",
    "\n",
    "In a **long format**, each row is a single observation, with one column specifying the \"type\" of observation and another column specifying the value. This format is often preferred in data visualization tools and can make data manipulation more straightforward in some contexts.\n",
    "\n",
    "Using the same sales data:\n",
    "\n",
    "```\n",
    "| Subject   | Month   | Sales |\n",
    "|-----------|---------|-------|\n",
    "| Product A | January | 10    |\n",
    "| Product A | February| 12    |\n",
    "| Product A | March   | 11    |\n",
    "| Product B | January | 15    |\n",
    "| Product B | February| 17    |\n",
    "| Product B | March   | 18    |\n",
    "```\n",
    "\n",
    "Each row corresponds to a single observation - a month's sales for a product.\n",
    "\n",
    "### Conversion with Pandas\n",
    "\n",
    "The Python library `pandas` offers intuitive functions to convert between wide and long formats:\n",
    "\n",
    "- **melt**: This function is used to change data from wide to long format.\n",
    "- **pivot** or **pivot_table**: These functions can convert data from long to wide format.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Wide to Long format using melt\n",
    "df_wide = pd.DataFrame({\n",
    "    'Subject': ['Product A', 'Product B'],\n",
    "    'January': [10, 15],\n",
    "    'February': [12, 17],\n",
    "    'March': [11, 18]\n",
    "})\n",
    "\n",
    "df_long = df_wide.melt(id_vars=['Subject'], \n",
    "                       value_vars=['January', 'February', 'March'], \n",
    "                       var_name='Month', \n",
    "                       value_name='Sales')\n",
    "\n",
    "# Long to Wide format using pivot\n",
    "df_wide_again = df_long.pivot(index='Subject', columns='Month', values='Sales').reset_index()\n",
    "```\n",
    "\n",
    "### When to Use Which Format?\n",
    "\n",
    "- **Wide Format**: Useful when comparing multiple variables for a single subject. It's often preferred in tools like Excel and in specific scenarios in data analysis where having data in separate columns is beneficial.\n",
    "  \n",
    "- **Long Format**: It's a preferred format for many data visualization tools like `seaborn` and `ggplot2` in R. It's also conducive for various types of data transformations and is considered a \"tidier\" way to represent data where each variable forms a column, each observation forms a row, and each type of observational unit forms a table.\n",
    "\n",
    "Understanding these data formats and how to convert between them can greatly improve your efficiency and effectiveness in data manipulation and analysis in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make the maize data long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mz_long = df_mz.melt(id_vars=['prod_year'], \n",
    "                       var_name='variable', \n",
    "                       value_name='value')\n",
    "\n",
    "#Lets add the crop\n",
    "df_mz_long['crop'] = 'maize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Tutorial 3, Part 1: Reading and Cleaning Data\n",
    "\n",
    "Now it your turn, do the following:\n",
    "1. Import the SA_soybean data and call it df_soy\n",
    "2. Correct the column names\n",
    "3. Correct the production year column\n",
    "4. Check the data types and convert them to numeric if necessary\n",
    "5. Rename the value column to something more descriptive\n",
    "6. Calculate the soybean yield for each year and add it to the data frame\n",
    "7. Make the data long, calling it df_soy_long\n",
    "8. Add a `crop_type` column to the long data frame and set it to `soybean`\n",
    "\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Joining Data\n",
    "\n",
    "We often want to combine different data sources to create a single dataset for analysis. For instance, we may have data on sales for different products in different regions, and we want to combine them to get a single dataset containing sales for all products across all regions. This process is called **data merging** or **data joining**.\n",
    "\n",
    "### Basics of Joining Data\n",
    "\n",
    "Pandas provides various ways to combine DataFrames including `.merge()` for database-style joins. The `merge()` function takes the following arguments:\n",
    "\n",
    "```python\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False)\n",
    "```\n",
    "\n",
    "- `left`: A DataFrame object.\n",
    "- `right`: Another DataFrame object.\n",
    "- `how`: One of 'left', 'right', 'outer', 'inner'. Defaults to 'inner', which keeps only the rows where the merge \"on\" value exists in both the left and right DataFrames.\n",
    "- `on`: Columns (names) to join on. Must be found in both the left and right DataFrame objects.\n",
    "- `left_on`: Columns from the left DataFrame to use as keys.\n",
    "- `right_on`: Columns from the right DataFrame to use as keys.\n",
    "- `left_index`: If `True`, use the index (row labels) from the left DataFrame as its join key(s). In other words, use the index to match rows from the left DataFrame with rows from the right DataFrame.\n",
    "- `right_index`: Same usage as `left_index` for the right DataFrame.\n",
    "\n",
    "Lets look at a couple of examples, but first, lets construct an example dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Name  Age in Years           Occupation  Age_squared  Name_length  \\\n",
      "0     Alice            25  Mechanical Engineer          625            5   \n",
      "1       Bob            30            Physician          900            3   \n",
      "2   Charlie            35               Artist         1225            7   \n",
      "3     David            40               Lawyer         1600            5   \n",
      "4       Eva            45            Scientist         2025            3   \n",
      "\n",
      "  Is_Elderly   Life_Stage  \n",
      "0         No        Young  \n",
      "1         No  Middle-aged  \n",
      "2         No  Middle-aged  \n",
      "3         No  Middle-aged  \n",
      "4        Yes  Middle-aged  \n",
      "\n",
      "The Merge data is:\n",
      "\n",
      "  Full Name  Salary\n",
      "0     Alice   50000\n",
      "1       Bob   60000\n",
      "2     Peter  100000\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "data = {\n",
    "    'Full Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "    'Age in Years': [25, 30, 35, 40, 45],\n",
    "    'Occupation': ['Mechanical Engineer', 'Physician', 'Artist', 'Lawyer', 'Scientist'],\n",
    "    'Age_squared': [625, 900, 1225, 1600, 2025],\n",
    "    'Name_length': [5, 3, 7, 5, 3],\n",
    "    'Is_Elderly': ['No', 'No', 'No', 'No', 'Yes'],\n",
    "    'Life_Stage': ['Young', 'Middle-aged', 'Middle-aged', 'Middle-aged', 'Middle-aged']\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "print(\"\")\n",
    "print(\"The Merge data is:\")\n",
    "print(\"\")\n",
    "data1 = {'Full Name': ['Alice', 'Bob','Peter'], 'Salary': [50000, 60000,100000]}\n",
    "df_add = pd.DataFrame(data1)\n",
    "print(df_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Name  Age in Years           Occupation  Age_squared  Name_length  \\\n",
      "0     Alice            25  Mechanical Engineer          625            5   \n",
      "1       Bob            30            Physician          900            3   \n",
      "\n",
      "  Is_Elderly   Life_Stage  Salary  \n",
      "0         No        Young   50000  \n",
      "1         No  Middle-aged   60000  \n"
     ]
    }
   ],
   "source": [
    "# Example 1: Inner join on `Full Name`\n",
    "merged_df = pd.merge(df, df_add, on='Full Name',how = 'inner')\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Name  Age in Years           Occupation  Age_squared  Name_length  \\\n",
      "0     Alice            25  Mechanical Engineer          625            5   \n",
      "1       Bob            30            Physician          900            3   \n",
      "2   Charlie            35               Artist         1225            7   \n",
      "3     David            40               Lawyer         1600            5   \n",
      "4       Eva            45            Scientist         2025            3   \n",
      "\n",
      "  Is_Elderly   Life_Stage   Salary  \n",
      "0         No        Young  50000.0  \n",
      "1         No  Middle-aged  60000.0  \n",
      "2         No  Middle-aged      NaN  \n",
      "3         No  Middle-aged      NaN  \n",
      "4        Yes  Middle-aged      NaN  \n"
     ]
    }
   ],
   "source": [
    "# Example 2: Left join\n",
    "left_joined_df = pd.merge(df, df_add, on='Full Name', how='left')\n",
    "print(left_joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Name  Age in Years           Occupation  Age_squared  Name_length  \\\n",
      "0     Alice          25.0  Mechanical Engineer        625.0          5.0   \n",
      "1       Bob          30.0            Physician        900.0          3.0   \n",
      "2   Charlie          35.0               Artist       1225.0          7.0   \n",
      "3     David          40.0               Lawyer       1600.0          5.0   \n",
      "4       Eva          45.0            Scientist       2025.0          3.0   \n",
      "5     Peter           NaN                  NaN          NaN          NaN   \n",
      "\n",
      "  Is_Elderly   Life_Stage    Salary  \n",
      "0         No        Young   50000.0  \n",
      "1         No  Middle-aged   60000.0  \n",
      "2         No  Middle-aged       NaN  \n",
      "3         No  Middle-aged       NaN  \n",
      "4        Yes  Middle-aged       NaN  \n",
      "5        NaN          NaN  100000.0  \n"
     ]
    }
   ],
   "source": [
    "# Example 3: Outer join\n",
    "outer_joined_df = pd.merge(df, df_add, on='Full Name', how='outer')\n",
    "print(outer_joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Tutorial 3, Part 2: Merging data\n",
    "\n",
    "Now it your turn, do the following:\n",
    "\n",
    "1. Import the `SA_season_type.csv` data and call it `df_season`\n",
    "2. Merge it with the df_maize_long data and call it `df_maize_long_season`\n",
    "3. Merge it with the df_soy_long data and call it `df_soy_long_season`\n",
    "\n",
    "**Note:** Make sure that you use the correct join type so that you do not lose data or introduce NA values.\n",
    "\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Data\n",
    "\n",
    "Concatenating data is the process of combining data from multiple sources into a single table. This is a common task in data science and data analysis scenarios, and `pandas` provides several functions to make this process straightforward.\n",
    "\n",
    "### Concatenating DataFrames\n",
    "\n",
    "The `pandas` function `concat()` can be used to concatenate two or more DataFrames into a single DataFrame. The function takes a list of DataFrames as an argument and returns a single DataFrame that combines the rows of the input DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Subject  January  February  March\n",
      "0  Product A       10        12     11\n",
      "1  Product B       15        17     18\n",
      "2  Product C       20        22     21\n",
      "3  Product D       25        27     28\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'Subject': ['Product A', 'Product B'],\n",
    "    'January': [10, 15],\n",
    "    'February': [12, 17],\n",
    "    'March': [11, 18]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'Subject': ['Product C', 'Product D'],\n",
    "    'January': [20, 25],\n",
    "    'February': [22, 27],\n",
    "    'March': [21, 28]\n",
    "})\n",
    "\n",
    "df3 = pd.concat([df1, df2], ignore_index=True)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets concatenate the maize and sunflower data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prod_year      variable   value   crop\n",
      "0   1980_81  area_planted  4488.0  maize\n",
      "1   1981_82  area_planted  4664.0  maize\n",
      "2   1982_83  area_planted  4680.0  maize\n",
      "3   1983_84  area_planted  4839.0  maize\n",
      "4   1984_85  area_planted  4502.0  maize\n",
      "['1980_81' '1981_82' '1982_83' '1983_84' '1984_85' '1985_86' '1986_87'\n",
      " '1987_88' '1988_89' '1989_90' '1990_91' '1991_92' '1992_93' '1993_94'\n",
      " '1994_95' '1995_96' '1996_97' '1997_98' '1998_99' '1999_00' '2000_01'\n",
      " '2001_02' '2002_03' '2003_04' '2004_05' '2005_06' '2006_07' '2007_08'\n",
      " '2008_09' '2009_10' '2010_11' '2011_12' '2012_13' '2013_14' '2014_15'\n",
      " '2015_16' '2016_17' '2017_18' '2018_19' '2019_20' '2020_21' '2021_22']\n"
     ]
    }
   ],
   "source": [
    "df_sunf = pd.read_csv(\"data/raw/SA_sunflower.csv\")\n",
    "\n",
    "df_sunf_long = df_sunf.melt(id_vars=['prod_year'],var_name='variable', value_name='value')\n",
    "\n",
    "df_sunf_long['crop'] = 'sunflower'\n",
    "\n",
    "df_mz_sunf = pd.concat([df_mz_long, df_sunf_long], ignore_index=True)\n",
    "print(df_mz_sunf.head())\n",
    "\n",
    "print(df_mz_sunf['prod_year'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "  - DataFrames can be saved to a variety of file formats.\n",
    "  - Example for writing to an Excel file:\n",
    "    ```python\n",
    "    df.to_excel('output.xlsx', sheet_name='Sheet1', index=False)\n",
    "    ```\n",
    "\n",
    "- **File Formats**:\n",
    "  - Pandas supports a variety of file formats including:\n",
    "    - Text formats such as CSV, JSON, and HTML.\n",
    "    - Binary formats such as Excel, HDF5, and Parquet.\n",
    "    - SQL databases like SQLite, PostgreSQL, and MySQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Tutorial 3, Part 3: Writing data\n",
    "\n",
    "Ok, please wrap up your tutorial by concatenating the maize, soybean and sunflower data and writing it to a csv file. Save it somewhere on your Google Drive where you can find it again since we will be using it in the next tutorial.\n",
    "\n",
    "----\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
